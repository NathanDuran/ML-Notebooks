{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "As we have seen, Perceptrons are only capable of solving *linearly separable* problems.\n",
    "To overcome this limitation we can connect Perceptrons together into a network,\n",
    "first proposed by Rumelhart. Mclelland & Hinton (1980's).\n",
    "Each one becomes a *Node* in the network and they are connected together into *Layers*.\n",
    "In standard Artificial Neural Network (ANN) architecture there is one *input*, one *output* and one or more *hidden* layers.\n",
    "Though *input* layer is a bit misleading, it doesn't actually do any computation, it is just the inputs to the network.\n",
    "\n",
    "![ANN](resources/ann.png \"ANN Image\")\n",
    "\n",
    "So outputs of hidden layers become the inputs to subsequent hidden layers, or the final output layer.\n",
    "Hidden nodes tend to learn different aspects of the problem space,\n",
    "building more complex decision boundaries and are therefore able to solve more complex problems.\n",
    "\n",
    "**Note:** The number of nodes in the input layer *must* equal the number of inputs/features in the data.\n",
    "The number of output nodes *must* equal the number of labels/classes in the data.\n",
    "The number of hidden layers and nodes in the layers is arbitrary,\n",
    "and selecting this architecture is part of building an ANN.\n",
    "\n",
    "### Differences Between Perceptrons and ANN\n",
    "\n",
    "Before we look at the algorithm for ANN we need to understand two key differences.\n",
    "\n",
    "#### 1. Activation Function\n",
    "\n",
    "Each node needs to output a *real number*, so the step function we used before (which outputs 0 or 1) will not work.\n",
    "Instead a *non-linear* function, like Sigmoid, which 'squashes' the output into a real number between 0 and 1.\n",
    "\n",
    "**Note:** Other activation functions are also used, such as Tahn and ReLu, but we will stick to Sigmoid.\n",
    "\n",
    "![Activation-Functions](resources/activation_functions.png \"Activation-Functions Image\")\n",
    "\n",
    "We need an activation function that outputs real numbers because:\n",
    "1. For output nodes, real numbers between 0 and 1 can be considered a **probability** of an input example belonging\n",
    "to a particular class.\n",
    "2. Hidden layer nodes need to produce *some* output, even if it is very small,\n",
    "so that we can calculate the error and update weights using **Backpropagation**.\n",
    "3. For Backpropagation the activation function needs to be differentiable, so we can calculate the gradient\n",
    "of the error with respect to the weights for **Gradient Descent**.\n",
    "\n",
    "#### 2. Backpropagation and Gradient Descent\n",
    "\n",
    "Perceptrons only have one layer, so from its output we can calculate the error it produces and use that to\n",
    "update the weight values.\n",
    "But now we have multiple layers what should the hidden nodes output be?\n",
    "What is the error and how much should we change the weights?\n",
    "\n",
    "Instead, we *share out the error* from the output nodes to the hidden nodes,\n",
    "and we do this in *proportion to the 'strength' of the output* that it produced - hence why we need *some* output.\n",
    "So we are *propagating* the error from the output nodes back up the network.\n",
    "This is achieved by calculating the derivative of the error from the previous layer with respect to the weights.\n",
    "Then use a similar weight update function that we did with Perceptrons:\n",
    "\n",
    "$change \\, in \\, weight = derivative \\times input \\times learning \\, rate$\n",
    "\n",
    "Why do we calculate the derivative of the error function? This is an algorithm called **Stochastic Gradient Descent**.\n",
    "We want to *minimise* the error produced by a weight.\n",
    "By calculating the derivative we get the *gradient* or the 'steepness' of a curve at that point (weight value).\n",
    "The larger the gradient the further we are from the minimum error (0 gradient).\n",
    "Again, the learning rate is how large a step we want to take towards the minimum error.\n",
    "\n",
    "![Gradient-Descent](resources/gradient_descent.png \"Gradient-Descent Image\")\n",
    "\n",
    "### ANN - Algorithm\n",
    "\n",
    "Similar to Perceptrons, ANN are trained in two 'phases'.\n",
    "The forward pass, where data is input into the network to produce an output.\n",
    "The backward pass, where the error in output is used to update the weights using Backpropagation and Gradient Descent.\n",
    "\n",
    "1. Set weights to random small values, for example in range [-0.5, 0.5]\n",
    "\n",
    "2. Set learning rate to a small value, usually less than 0.5\n",
    "\n",
    "3. For each training example in the dataset i.e one 'epoch'\n",
    "\n",
    "    // Forward Propagation\n",
    "    \n",
    "    A. For each node in the layer and each layer in turn:\n",
    "    \n",
    "    Sum inputs multiplied by weights\n",
    "        \n",
    "    $sum = \\sum\\limits_{i=0}^{n} w_i \\times x_i$\n",
    "\n",
    "    Calculate Sigmoid (activation) of the sum\n",
    "\n",
    "    $activation = \\sigma(sum)$\n",
    "    \n",
    "    // Backpropagation\n",
    "    \n",
    "    B. For each node in the layer and each layer in turn **going backwards**:\n",
    "        \n",
    "    Calculate the error and derivative, first the output layer then hidden.\n",
    "        \n",
    "    $output \\, \\epsilon = target \\, output - activation$\n",
    "    \n",
    "    $output \\, \\delta = output \\, \\epsilon \\times sigmoid \\, derivative(activation)$\n",
    "    \n",
    "    $hidden \\, layer \\, \\epsilon = output \\, \\delta \\times output \\, weights$\n",
    "    \n",
    "    $hidden \\, layer \\, \\delta = hidden \\, layer \\, \\epsilon \\times sigmoid \\, derivative(hidden \\, layer \\, activation)$\n",
    "        \n",
    "    C. Update all the weights **at the same time**, with learning rate, inputs and gradients:\n",
    "    \n",
    "    $change \\, in \\, weight = learning \\, rate \\times input \\times \\delta$\n",
    "    \n",
    "4. Repeat from step 3 until error is as small as possible, or (more likely) for the number of training epochs.\n",
    "\n",
    "### ANN - Solving XOR\n",
    "\n",
    "As an introduction to the ANN algorithm, and to give you an intuition for how different nodes and layers in the network\n",
    "learn different aspects of the problem space, we are going to look at how a small network can solve the XOR problem.\n",
    "Take a look at the following diagram.\n",
    "The hidden nodes both learn different logical functions (AND and OR), the output node learns OR, so in combination\n",
    "they have solved XOR!\n",
    "\n",
    "![ANN-XOR](resources/ann_xor.png \"ANN-XOR Image\")\n",
    "\n",
    "First we will define a NeuralNetwork class that has the weight variables and functions like train, predict and sigmoid.\n",
    "Then the training data is loaded and we can call the train function, which returns the trained weights.\n",
    "\n",
    "As it trains you should see the error *decrease* and the accuracy *increase*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Import some needed modules\n",
    "from IPython.display import HTML, display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns;sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline\n",
    "np.random.seed(3)\n",
    " \n",
    "def generate_decision_boundary(x, pred_func, model):\n",
    "    \"\"\" Generates predictions for each point of a grid. \n",
    "    This function has nothing to do with neural networks.\"\"\"\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = x[:, 0].min() - .5, x[:, 0].max() + .5\n",
    "    y_min, y_max = x[:, 1].min() - .5, x[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Predict the function value for the whole grid\n",
    "    z = pred_func(np.c_[xx.ravel(), yy.ravel()], model)\n",
    "    z = z.reshape(xx.shape)\n",
    "    return xx, yy, z\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "\n",
    "        # Set the weights to small random values in the range -1 to 1\n",
    "        self.hidden1_w1 = np.random.uniform(-1, 1)\n",
    "        self.hidden1_w2 = np.random.uniform(-1, 1)\n",
    "        self.hidden1_bw = np.random.uniform(-1, 1)\n",
    "    \n",
    "        self.hidden2_w1 = np.random.uniform(-1, 1)\n",
    "        self.hidden2_w2 = np.random.uniform(-1, 1)\n",
    "        self.hidden2_bw = np.random.uniform(-1, 1)\n",
    "    \n",
    "        self.out_w1 = np.random.uniform(-1, 1)\n",
    "        self.out_w2 = np.random.uniform(-1, 1)\n",
    "        self.out_bw = np.random.uniform(-1, 1)\n",
    "        \n",
    "        self.model = {'hidden1': [self.hidden1_w1, self.hidden1_w2, self.hidden1_bw],\n",
    "                 'hidden2': [self.hidden2_w1, self.hidden2_w2, self.hidden2_bw],\n",
    "                 'out': [self.out_w1, self.out_w2, self.out_bw]}\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_deriv(x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def train(self, inputs, target_outputs, training_epochs, learning_rate):\n",
    "        \n",
    "        # Array to store the decision boundaries as the model trains\n",
    "        decision_boundary = []\n",
    "        \n",
    "        # Each epoch will loop over the training data once\n",
    "        for epoch in range(training_epochs + 1):\n",
    "            epoch_error = []\n",
    "            \n",
    "            # Loop over all of the input examples\n",
    "            for i in range(len(inputs)):\n",
    "                \n",
    "                \"\"\" Forward Pass - propagates input data through the network. \"\"\"\n",
    "                # Input layer is just the input data\n",
    "                input_layer = inputs\n",
    "                \n",
    "                # Hidden layer sigmoid(W * X + b)\n",
    "                hidden1_sum = (input_layer[i][0] * self.hidden1_w1) + (input_layer[i][1] * self.hidden1_w2) + self.hidden1_bw\n",
    "                hidden1_output = self.sigmoid(hidden1_sum)\n",
    "                \n",
    "                hidden2_sum = (input_layer[i][0] * self.hidden2_w1) + (input_layer[i][1] * self.hidden2_w2) + self.hidden2_bw\n",
    "                hidden2_output = self.sigmoid(hidden2_sum)  \n",
    "                \n",
    "                # Output layer sigmoid(W * X + b)\n",
    "                out_sum = (hidden1_output * self.out_w1) + (hidden2_output * self.out_w2) + self.out_bw\n",
    "                output = self.sigmoid(out_sum)\n",
    "                \n",
    "                \"\"\" Backpropagation - propagates the error backwards through the network. \"\"\"\n",
    "                # Calculate output error (target output - actual output)\n",
    "                error = target_outputs[i] - output\n",
    "                epoch_error.append(error) # Also keep track of total error for this epoch\n",
    "    \n",
    "                # Calculate the derivative of the error with respect to the weights\n",
    "                out_delta = error * self.sigmoid_deriv(output)\n",
    "                out_bias_delta = error\n",
    "    \n",
    "                # Calculate hidden layer errors (from the output layers weights and gradient)\n",
    "                hidden1_error = out_delta * self.out_w1\n",
    "                hidden2_error = out_delta * self.out_w2\n",
    "    \n",
    "                # Calculate the derivative of the error with respect to the weights\n",
    "                hidden1_w1_delta = hidden1_error * self.sigmoid_deriv(hidden1_output)\n",
    "                hidden1_w2_delta = hidden1_error * self.sigmoid_deriv(hidden1_output)\n",
    "                hidden1_bw_delta = hidden1_error\n",
    "    \n",
    "                hidden2_w1_delta = hidden2_error * self.sigmoid_deriv(hidden2_output)\n",
    "                hidden2_w2_delta = hidden2_error * self.sigmoid_deriv(hidden2_output)\n",
    "                hidden2_bw_delta = hidden2_error\n",
    "                \n",
    "                \"\"\" Update the Weights - update the weights using the error gradients, input and learning rate.\"\"\"\n",
    "                # Change in weight = learning rate * layers input * layers gradient\n",
    "                self.out_w1 += learning_rate * hidden1_output * out_delta\n",
    "                self.out_w2 += learning_rate * hidden2_output * out_delta\n",
    "                self.out_bw += learning_rate * out_bias_delta\n",
    "    \n",
    "                self.hidden1_w1 += learning_rate * input_layer[i][0] * hidden1_w1_delta\n",
    "                self.hidden1_w2 += learning_rate * input_layer[i][1] * hidden1_w2_delta\n",
    "                self.hidden1_bw += learning_rate * hidden1_bw_delta\n",
    "    \n",
    "                self.hidden2_w1 += learning_rate * input_layer[i][0] * hidden2_w1_delta\n",
    "                self.hidden2_w2 += learning_rate * input_layer[i][1] * hidden2_w2_delta\n",
    "                self.hidden2_bw += learning_rate * hidden2_bw_delta\n",
    "            \n",
    "            # Every 100 epochs, calculate error and accuracy    \n",
    "            if epoch % 100 == 0:\n",
    "                # Calculate the mean squared error\n",
    "                mean_error = round(np.square(epoch_error).mean(), 5) \n",
    "                \n",
    "                # Make predictions on the data\n",
    "                predictions = self.predict(inputs, self.model)\n",
    "                # Count the number of correct predictions\n",
    "                correct_predictions = np.count_nonzero(target_outputs == np.rint(predictions))\n",
    "                \n",
    "                # Calculate the accuracy     \n",
    "                accuracy = (100 / len(inputs)) * correct_predictions\n",
    "                print(\"Epoch: \" + str(epoch) + \" Error: \" + str(mean_error) + \" Accuracy: \" + str(accuracy) + \"%\")\n",
    "                \n",
    "                # Calculate and store decision boundary\n",
    "                _, _, boundary = generate_decision_boundary(inputs, self.predict, self.model)\n",
    "                decision_boundary.append({'boundary': boundary, 'epoch': epoch, 'error': mean_error, 'accuracy': accuracy})\n",
    "            \n",
    "            # Update the model\n",
    "            self.model = {'hidden1': [self.hidden1_w1, self.hidden1_w2, self.hidden1_bw],\n",
    "                 'hidden2': [self.hidden2_w1, self.hidden2_w2, self.hidden2_bw],\n",
    "                 'out': [self.out_w1, self.out_w2, self.out_bw]}\n",
    "    \n",
    "        return self.model, decision_boundary\n",
    "    \n",
    "    def predict(self, x, model):\n",
    "        \"\"\" Generates predictions for the whole network. \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        # Loop over all of the input examples\n",
    "        for i in range(len(x)):\n",
    "            # Calculate output\n",
    "            hidden1_sum = (x[i][0] * model['hidden1'][0]) + (x[i][1] * model['hidden1'][1]) + model['hidden1'][2]\n",
    "            hidden1_output = self.sigmoid(hidden1_sum)\n",
    "    \n",
    "            hidden2_sum = (x[i][0] * model['hidden2'][0]) + (x[i][1] * model['hidden2'][1]) + model['hidden2'][2]\n",
    "            hidden2_output = self.sigmoid(hidden2_sum)  \n",
    "    \n",
    "            out_sum = (hidden1_output * model['out'][0]) + (hidden2_output * model['out'][1]) + model['out'][2]\n",
    "            output = self.sigmoid(out_sum)\n",
    "            \n",
    "            # Store predictions in an array\n",
    "            predictions.append(output)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def node_predict(self, x, node):\n",
    "        \"\"\" Generates predictions for a single node. \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        # Loop over all of the input examples\n",
    "        for i in range(len(x)):\n",
    "            # Calculate output\n",
    "            weight_sum = (x[i][0] * node[0]) + (x[i][1] * node[1]) + node[2]\n",
    "            # output = self.sigmoid(weight_sum)\n",
    "            output = 0 if weight_sum < 0 else 1  # Using step function here to make graphs easier to read\n",
    "            \n",
    "            # Store predictions in an array\n",
    "            predictions.append(output)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Training data\n",
    "train_x = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 0],\n",
    "                   [1, 1]])\n",
    "\n",
    "train_y = np.array([0, 1, 1, 0]) # XOR\n",
    "\n",
    "# Number of training epochs\n",
    "learning_rate = 1  # The learning rate usually SHOULD NOT be this high! (Why do you think it is?)\n",
    "# Set the learning rate and number of training epochs\n",
    "num_epochs = 1500\n",
    "\n",
    "# Create ann and call train method\n",
    "ann = NeuralNetwork()\n",
    "trained_model, decision_boundaries = ann.train(train_x, train_y, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ANN - Decision Boundary\n",
    "\n",
    "Once the model is trained we can use the weights (and a little bit of trickery) to plot the decision boundary for each\n",
    "node. You should see that each node has learned a different function, or a different aspect of the problem space,\n",
    "as was shown in the diagram above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create decision boundaries for each node in the network\n",
    "x_points, y_points, h1_pred = generate_decision_boundary(train_x, ann.node_predict, trained_model['hidden1'])\n",
    "_, _, h2_pred = generate_decision_boundary(train_x, ann.node_predict, trained_model['hidden2'])\n",
    "_, _, ann_pred = generate_decision_boundary(train_x, ann.predict, trained_model)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "figure, ax = plt.subplots(2, 2, figsize=(16, 12))\n",
    "[axi.set_axis_off() for axi in ax.ravel()]\n",
    "ax[0, 0].contourf(x_points, y_points, h1_pred, alpha = 0.6, cmap='Spectral')\n",
    "ax[0, 0].scatter(train_x[:, 0], train_x[:, 1], c=train_y.ravel(), s=50, cmap='RdYlGn')\n",
    "ax[0, 0].title.set_text('Hidden Node 1')\n",
    "ax[0, 1].contourf(x_points, y_points, h2_pred, alpha = 0.6, cmap='Spectral')\n",
    "ax[0, 1].scatter(train_x[:, 0], train_x[:, 1], c=train_y.ravel(), s=50, cmap='RdYlGn')\n",
    "ax[0, 1].title.set_text('Hidden Node 2')\n",
    "ax[1, 0].contourf(x_points, y_points, ann_pred, alpha = 0.6, cmap='Spectral')\n",
    "ax[1, 0].scatter(train_x[:, 0], train_x[:, 1], c=train_y.ravel(), s=50, cmap='RdYlGn')\n",
    "ax[1, 0].title.set_text('Output Node')\n",
    "\n",
    "# This function animates the decision boundaries that were saved as the model trained\n",
    "def animate(i):\n",
    "    ax[1, 1].clear()\n",
    "    contour = ax[1, 1].contourf(x_points, y_points, decision_boundaries[i]['boundary'], alpha = 0.6, cmap='Spectral')\n",
    "    scatter = ax[1, 1].scatter(train_x[:, 0], train_x[:, 1], c=train_y.ravel(), s=50, cmap='RdYlGn')\n",
    "    epoch, error, acc = decision_boundaries[i]['epoch'], decision_boundaries[i]['error'], decision_boundaries[i]['accuracy']\n",
    "    ax[1, 1].title.set_text('Epoch: {} Error: {:.4f} Accuracy: {}%'.format(epoch, error, acc))\n",
    "    ax[1, 1].axis('off')\n",
    "    return contour, scatter\n",
    "animation = animation.FuncAnimation(figure, animate, interval=250, repeat_delay=1000, frames=len(decision_boundaries))\n",
    "plt.tight_layout()\n",
    "# animation.save(os.path.join('output','ann_xor_decision_boundary.gif'), writer=animation.PillowWriter(fps=5), dpi='figure')\n",
    "plt.close()\n",
    "display(HTML(animation.to_jshtml()))\n",
    "\n",
    "# Create a table that shows the inputs and outputs of each node\n",
    "table = pd.DataFrame({'x1': train_x[:, 0], 'x2': train_x[:, 1], 'XOR': train_y,\n",
    "                   'Hidden 1': ann.node_predict(train_x, trained_model['hidden1']),\n",
    "                   'Hidden 2': ann.node_predict(train_x, trained_model['hidden2']),\n",
    "                   'Output': np.rint(ann.predict(train_x, trained_model)).astype(int),\n",
    "                   'Output (Raw)': ann.predict(train_x, trained_model)})\n",
    "\n",
    "table\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}