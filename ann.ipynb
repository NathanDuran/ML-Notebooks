{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Artificial Neural Networks and the Iris Data\n",
    "\n",
    "The Iris Flower data set contains 150 examples from three species Iris Setosa, Iris Virginica and Iris Versicolor.\n",
    "There are 50 examples of each species and each example has four measurements (features) Sepal Length, Sepal Width,\n",
    "Petal Length and Petal Width.\n",
    "The Iris data is often used as an example for machine learning classifiers and we are going to build and test an ANN\n",
    "to classify the data i.e. given a set of measurements, what is the species?\n",
    "\n",
    "![Iris-image](resources/iris_image.png \"Iris-image Image\")\n",
    "\n",
    "Real data very rarely comes in a format that is suitable for input to a machine learning algorithm.\n",
    "So first we need to prepare the data ready for classification.\n",
    "It is also often useful to visualise the data because this might help us select what kind of classifier is suitable\n",
    "and predict how well they might perform.\n",
    "\n",
    "We also need to replace the species labels with numbers and convert them to numbers.\n",
    "In this case we are going to use ‘one-hot encoding’,\n",
    "which means each species label will be replaced with a set of binary values which indicate which of the three\n",
    "species it is i.e 'Iris-setosa' = 1 0 0, 'Iris-versicolor' = 0 1 0 and 'Iris-virginica' = 0 0 1.\n",
    "\n",
    "We also need to get all of the features from the relevant columns and split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns ;sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "# Read data from csv\n",
    "iris_data = pd.read_csv(\"data/iris.csv\")\n",
    "iris_data.drop(\"Id\", axis=1, inplace=True)\n",
    "\n",
    "# Plot the various combinations on 2D graph\n",
    "iris_data_plt = sns.pairplot(iris_data, hue=\"Species\")\n",
    "\n",
    "# Replace the species with 0, 1 or 2 as appropriate\n",
    "iris_data['Species'].replace(['Iris-setosa', 'Iris-virginica', 'Iris-versicolor'], [0, 1, 2], inplace=True)\n",
    "\n",
    "# Get labels and encode to one-hot\n",
    "labels = iris_data['Species'].to_numpy()\n",
    "labels = np.eye(np.max(labels) + 1)[labels]\n",
    "\n",
    "# Get Features\n",
    "feature_names = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
    "features = iris_data[feature_names].to_numpy()\n",
    "\n",
    "# Split data to training and test data, 2/3 for training and 1/3 for testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.33)\n",
    "\n",
    "# Show the first 5 iris examples\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ANN - Implementation\n",
    "\n",
    "The previous neural network implementation was a little long winded.\n",
    "If we had to manually add new variables for each weight/node it would be quite unmanageable.\n",
    "For example with 4 inputs and 6 hidden nodes (+ 6 bias) = 30 weight variables for just one layer.\n",
    "Instead we can represent the entire layer as a matrix, in this case the hidden layer will be a 4x6 matrix.\n",
    "This also allows us to perform the calculations on the entire layer at once, rather than using loops,\n",
    "which is much more efficient and easier to code.\n",
    "\n",
    "This code will create a network with a single hidden layer.\n",
    "The forward and backward passes through the network have also been split into separate functions, so that they can\n",
    "be called independently within the train and predict functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, num_inputs, num_outputs, num_hidden_nodes):\n",
    "        # Get the number of inputs, outputs and hidden nodes\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_ouputs = num_outputs\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        \n",
    "        # Initialise weights in the range -0.5 to 0.5\n",
    "        # Hidden layer weights with shape (number of input features x number of hidden nodes)\n",
    "        self.hidden_weights = np.random.uniform(-0.5, 0.5, size=(self.num_inputs, self.num_hidden_nodes))\n",
    "        self.hidden_bias = np.random.uniform(-0.5, 0.5, size=(1, self.num_hidden_nodes))\n",
    "        # Output layer weights with shape (number of hidden nodes x number of output classes)\n",
    "        self.output_weights = np.random.uniform(-0.5, 0.5, size=(self.num_hidden_nodes, self.num_ouputs))\n",
    "        self.output_bias = np.random.uniform(-0.5, 0.5, size=(1, self.num_ouputs))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_deriv(x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\" Forward Pass - propagates input data through the network.\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): The input data to propagate through the network. shape=[num_examples, num_features]\n",
    "        \n",
    "        Returns:\n",
    "            hidden_output (np.array): Output (activation) of hidden layer. shape=[num_examples, num_hidden_nodes]\n",
    "            output (np.array): Output (activation of output layer. shape=[num_examples, num_outputs]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Input layer is just the input data\n",
    "        input_layer = x\n",
    "        # Hidden layer sigmoid(W * X + b)\n",
    "        hidden_output = self.sigmoid(np.dot(input_layer, self.hidden_weights) + self.hidden_bias)\n",
    "        # Output layer sigmoid(W * X + b)\n",
    "        output = self.sigmoid(np.dot(hidden_output, self.output_weights) + self.output_bias)\n",
    "\n",
    "        # Return both layers output\n",
    "        return hidden_output, output\n",
    "    \n",
    "    def backward_pass(self, x, y, hidden_output, output, lr):\n",
    "        \"\"\" Backpropagation - propagates the error backwards through the network.\n",
    "                \n",
    "        Args:\n",
    "            x (np.array): The input data to propagate through the network. shape=[num_examples, num_features]\n",
    "            y (np.array): The input data target labels. shape=[num_examples, num_outputs]\n",
    "            hidden_output (np.array): Output (activation) of hidden layer. shape=[num_examples, num_hidden_nodes]\n",
    "            output (np.array): Output (activation of output layer. shape=[num_examples, num_outputs]\n",
    "            lr (float): The learning rate, amount to adjust weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate output layer error\n",
    "        output_error = y - output\n",
    "    \n",
    "        # Calculate the derivative of the error with respect to the weights\n",
    "        # Note: just need this layers error for the bias\n",
    "        output_layer_delta = output_error * self.sigmoid_deriv(output)\n",
    "        output_bias_delta = np.sum(output_error, axis=0)\n",
    "        \n",
    "        # Calculate hidden layer errors (from the output layers weights and gradient)\n",
    "        hidden_layer_error = output_layer_delta.dot(self.output_weights.T)\n",
    "        \n",
    "        # Calculate the derivative of the error with respect to the weights\n",
    "        # Note: just need this layers error for the bias\n",
    "        hidden_layer_delta = hidden_layer_error * self.sigmoid_deriv(hidden_output)\n",
    "        hidden_bias_delta = np.sum(hidden_layer_error, axis=0)\n",
    "         \n",
    "        \"\"\" Update the Weights - update the weights using the error gradients, input and learning rate.\"\"\"\n",
    "        # Change in weight = learning rate * layers input * layers gradient\n",
    "        self.output_weights += lr * hidden_output.T.dot(output_layer_delta)\n",
    "        self.output_bias += lr * output_bias_delta\n",
    "        \n",
    "        self.hidden_weights += lr * x.T.dot(hidden_layer_delta)\n",
    "        self.hidden_bias += lr * hidden_bias_delta\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\" Generate predictions on input data.\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): The input data to make predictions on. shape=[num_examples, num_features]\n",
    "        \n",
    "        Returns:\n",
    "            preds (np.array): The predictions for the input data. shape=[num_examples]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pass the data through the network and generate outputs\n",
    "        _, outputs = self.forward_pass(x)\n",
    "        \n",
    "        # Prediction is the output node with the highest value\n",
    "        preds = np.argmax(outputs, axis=1)\n",
    "        return preds\n",
    "    \n",
    "    def train(self, x, y, lr=0.01, epochs=200, eval_epochs=10):\n",
    "        \"\"\" Train the network on the input data.\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): The input data to propagate through the network. shape=[num_examples, num_features]\n",
    "            y (np.array): The input data target labels. shape=[num_examples, num_outputs]\n",
    "            lr (float): The learning rate, amount to adjust weights.\n",
    "            epochs (int): Number of epochs to train the network. Default=200\n",
    "            eval_epochs (int): Evaluate the network on training data every this many epochs. Default=10\n",
    "            \n",
    "        Returns:\n",
    "            train_errors (array): List of errors recorded during training.\n",
    "            train_accuracies (array): List of accuracies recorded during training.\n",
    "        \"\"\"\n",
    "        \n",
    "        # For recording error and accuracy - for graph later\n",
    "        train_errors, train_accuracies = [], []\n",
    "        \n",
    "        # Train for number of epochs\n",
    "        for epoch in range(epochs + 1):\n",
    "            # Forward pass\n",
    "            hidden_output, outputs = self.forward_pass(x)\n",
    "            # Backward pass/weight update\n",
    "            self.backward_pass(x, y, hidden_output, outputs, lr)\n",
    "            \n",
    "            # Every 'eval_epochs' record error and accuracy on training and test set\n",
    "            if (epoch % eval_epochs) == 0:\n",
    "                \n",
    "                # Mean squared error over all errors this epoch\n",
    "                error = np.square(y - outputs).mean() \n",
    "                train_errors.append(error)\n",
    "   \n",
    "                # Get the prediction i.e. the output with the highest value\n",
    "                predictions = self.predict(x)\n",
    "                # Get the actual labels\n",
    "                actual_labels = np.argmax(y, axis=1)\n",
    "\n",
    "                # If they match the prediction was correct\n",
    "                correct_predictions = np.sum(predictions == actual_labels)\n",
    "                accuracy = (100 / len(train_x)) * correct_predictions\n",
    "                train_accuracies.append(accuracy)\n",
    "                \n",
    "                print(\"Epoch: \" + str(epoch) + \" Error: \" + str(round(error, 5)) + \" Accuracy: \" + str(round(accuracy, 3)) + \"%\")\n",
    "\n",
    "        return train_errors, train_accuracies\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Iris - Train\n",
    "\n",
    "First the network architecture needs to be defined, by specifying the number of input, hidden and output nodes.\n",
    "Then we can call the train function with the number of epochs and learning rate.\n",
    "\n",
    "Every 10 epochs we will record the mean squared error and accuracy of predictions.\n",
    "You should see the error drop and accuracy increase smoothly(ish) over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "# Number of training epochs\n",
    "num_epochs = 200\n",
    "\n",
    "# Network architecture parameters\n",
    "num_features = 4\n",
    "num_classes = 3\n",
    "num_hidden_nodes = 6\n",
    "\n",
    "# Build the network\n",
    "ann = NeuralNetwork(num_features, num_classes, num_hidden_nodes)\n",
    "# Call the train function\n",
    "train_errors, train_accuracies = ann.train(train_x, train_y, learning_rate, num_epochs)\n",
    "\n",
    "# Plot the training accuracy and error\n",
    "x_range = [i*10 for i in range(len(train_errors))]\n",
    "figure, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.lineplot(x=x_range, y=train_accuracies, color='b', ax=ax[0])\n",
    "ax[0].title.set_text(\"Accuracy\")\n",
    "sns.lineplot(x=x_range, y=train_errors, color='b', ax=ax[1])\n",
    "ax[1].title.set_text(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris - Test\n",
    "\n",
    "Now we can test the trained model by making predictions on the test data.\n",
    "The predict function returns a list of predictions for each example.\n",
    "To calculate accuracy we just need to compare the predictions to the actual labels and count how many times they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "test_predictions = ann.predict(test_x)\n",
    "# Get the actual labels\n",
    "actual_labels = np.argmax(test_y, axis=1)\n",
    "\n",
    "# If they match the prediction was correct\n",
    "correct_predictions = np.sum(test_predictions == actual_labels)\n",
    "test_accuracy = (100 / len(test_x)) * correct_predictions\n",
    "print('Test Accuracy: ' + str(test_accuracy) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ANN - Wheat Seeds Data\n",
    "\n",
    "The Wheat Seeds Dataset involves the prediction of species given measurements of seeds from different varieties of wheat.\n",
    "It is a 3-class classification problem. The number of examples for each class is balanced and there are 210 examples\n",
    "with 7 feature variables.\n",
    "\n",
    "The data is being processed in a similar way as the Iris data, but you should see that it is much harder to separate\n",
    "the different classes of wheat seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from csv\n",
    "wheat_data = pd.read_csv(\"data/wheat_seeds.csv\")\n",
    "wheat_data.drop(\"Id\", axis=1, inplace=True)\n",
    "\n",
    "# Plot the various combinations on 2D graph\n",
    "wheat_data_plt = sns.pairplot(wheat_data, hue=\"Class\", diag_kws={'bw':1.0})\n",
    "\n",
    "# Replace the class with 0, 1 or 2 as appropriate\n",
    "wheat_data['Class'].replace(['class-1', 'class-2', 'class-3'], [0, 1, 2], inplace=True)\n",
    "\n",
    "# Get labels and encode to one-hot\n",
    "labels = wheat_data['Class'].to_numpy()\n",
    "labels = np.eye(np.max(labels) + 1)[labels]\n",
    "\n",
    "# Get Features\n",
    "feature_names = ['Area', 'Perimeter', 'Compactness', 'Length of Kernel', 'Width of Kernel', 'Asymmetry Coefficient', 'Length of Kernel Groove']\n",
    "features = wheat_data[feature_names].to_numpy()\n",
    "\n",
    "# Split data to training and test data, 2/3 for training and 1/3 for testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.33)\n",
    " \n",
    "# Show the first 5 examples\n",
    "wheat_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Wheat Seeds - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "learning_rate = 0.001\n",
    "# Number of training epochs\n",
    "num_epochs = 2000\n",
    "\n",
    "# Network architecture parameters\n",
    "num_features = 7\n",
    "num_classes = 3\n",
    "num_hidden_nodes = 8\n",
    "\n",
    "# Build the network\n",
    "ann = NeuralNetwork(num_features, num_classes, num_hidden_nodes)\n",
    "# Call the train function\n",
    "train_errors, train_accuracies = ann.train(train_x, train_y, learning_rate, num_epochs, eval_epochs=100)\n",
    "\n",
    "# Plot the training accuracy and error\n",
    "x_range = [i*10 for i in range(len(train_errors))]\n",
    "figure, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.lineplot(x=x_range, y=train_accuracies, color='b', ax=ax[0])\n",
    "ax[0].title.set_text(\"Accuracy\")\n",
    "sns.lineplot(x=x_range, y=train_errors, color='b', ax=ax[1])\n",
    "ax[1].title.set_text(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Wheat Seeds - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "test_predictions = ann.predict(test_x)\n",
    "# Get the actual labels\n",
    "actual_labels = np.argmax(test_y, axis=1)\n",
    "\n",
    "# If they match the prediction was correct\n",
    "correct_predictions = np.sum(test_predictions == actual_labels)\n",
    "test_accuracy = (100 / len(test_x)) * correct_predictions\n",
    "print('Test Accuracy: ' + str(test_accuracy) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}