{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Read data from csv\n",
    "iris = pd.read_csv(\"data/Iris.csv\")\n",
    "print(iris.head())\n",
    "\n",
    "# Plot the various combinations of 2D graph\n",
    "#g = sns.pairplot(iris.drop(\"Id\", axis=1), hue=\"Species\")\n",
    "\n",
    "# Replace the species with 0, 1 or 2 as appropriate\n",
    "iris['Species'].replace(['Iris-setosa', 'Iris-virginica', 'Iris-versicolor'], [0, 1, 2], inplace=True)\n",
    "\n",
    "# Get labels, flatten and encode to one-hot\n",
    "columns = ['Species']\n",
    "labels = pd.DataFrame(iris, columns=columns).to_numpy()\n",
    "labels = labels.flatten()\n",
    "labels = np.eye(np.max(labels) + 1)[labels]\n",
    "\n",
    "# Get Features\n",
    "columns = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
    "features = pd.DataFrame(iris, columns=columns).to_numpy()\n",
    "\n",
    "# Split data to training and test data, 2/3 for training and 1/3 for testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "train and test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sigmoid and its derivative\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 200\n",
    "\n",
    "# Network architecture parameters\n",
    "num_features = len(train_x[0])\n",
    "num_classes = len(train_y[0])\n",
    "num_hidden_nodes = 8\n",
    "\n",
    "# Initialise weights in the range -1 to 1\n",
    "# np.random.seed(1)\n",
    "# Hidden layer weights with shape = number of input features x number of hidden nodes\n",
    "hidden_weights = np.random.uniform(-1, 1, size=(num_features, num_hidden_nodes))\n",
    "hidden_bias = np.random.uniform(-1, 1, size=(1, num_hidden_nodes))\n",
    "# Output layer weights with shape = number of hidden nodes x number of output classes\n",
    "output_weights = np.random.uniform(-1, 1, size=(num_hidden_nodes, num_classes))\n",
    "output_bias = np.random.uniform(-1, 1, size=(1, num_classes))\n",
    "\n",
    "# For recording error and accuracy - for graph later\n",
    "training_errors = []\n",
    "testing_errors = []\n",
    "training_accuracies = []\n",
    "testing_accuracies = []\n",
    "\n",
    "# Train for number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Forward pass\n",
    "    input_layer = train_x\n",
    "    # sigmoid( (W * X) + b)\n",
    "    hidden_layer = sigmoid(np.dot(input_layer, hidden_weights) + hidden_bias)\n",
    "    output_layer = sigmoid(np.dot(hidden_layer, output_weights) + output_bias)\n",
    "\n",
    "    \"\"\" Backpropagation using gradient descent \"\"\" \n",
    "    # Calculate output layer error\n",
    "    output_layer_error = (train_y - output_layer)\n",
    "\n",
    "    # Calculate output layer derivative Note: that we just need this layers error for the bias\n",
    "    output_layer_delta = output_layer_error * sigmoid_deriv(output_layer)\n",
    "    output_bias_delta = np.sum(output_layer_error, axis=0)\n",
    "    \n",
    "    # Calculate hidden layer error (from the output layers weights and derivative\n",
    "    hidden_layer_error = output_layer_delta.dot(output_weights.T)\n",
    "    # Calculate hidden layer derivative Note: that we just need this layers error for the bias\n",
    "    hidden_layer_delta = hidden_layer_error * sigmoid_deriv(hidden_layer)\n",
    "    hidden_bias_delta = np.sum(hidden_layer_error, axis=0)\n",
    "\n",
    "    # Update the weights (learning rate X layers input X layers derivative)\n",
    "    output_weights += learning_rate * hidden_layer.T.dot(output_layer_delta)\n",
    "    output_bias += learning_rate * output_bias_delta\n",
    "    \n",
    "    hidden_weights += learning_rate * input_layer.T.dot(hidden_layer_delta)\n",
    "    hidden_bias += learning_rate * hidden_bias_delta\n",
    "\n",
    "    # Every 100 epochs record error and accuracy during training\n",
    "    if (epoch % 10) == 0:\n",
    "        \n",
    "        # Mean squared error over all errors this epoch\n",
    "        error = np.square(output_layer_error).mean() \n",
    "        training_errors.append(error)\n",
    "\n",
    "        accuracy_count = 0\n",
    "        for i in range(len(output_layer)):\n",
    "          \n",
    "            # Get the prediction i.e. the output with the highest value\n",
    "            prediction = np.argmax(output_layer[i])\n",
    "            # Get the actual label\n",
    "            actual_label = np.argmax(train_y[i])\n",
    "            \n",
    "            # If they match the prediction was correct\n",
    "            if prediction == actual_label:\n",
    "                accuracy_count += 1\n",
    "        accuracy = (len(train_x) / 100) * accuracy_count\n",
    "        training_accuracies.append(accuracy)\n",
    "               \n",
    "        # Forward pass\n",
    "        test_hidden = sigmoid(np.dot(test_x, hidden_weights) + hidden_bias)\n",
    "        test_output = sigmoid(np.dot(test_hidden, output_weights) + output_bias)\n",
    "        test_output_error = test_y - test_output\n",
    "        # Mean squared error over all errors\n",
    "        test_error = np.square(test_output_error).mean()\n",
    "        testing_errors.append(test_error)\n",
    "\n",
    "        test_accuracy_count = 0\n",
    "        for j in range(len(test_output)):\n",
    "\n",
    "            # Get the prediction i.e. the output with the highest value\n",
    "            test_prediction = np.argmax(test_output[j])\n",
    "            # Get the actual label\n",
    "            actual_label = np.argmax(test_y[j])\n",
    "\n",
    "            # If they match the prediction was correct\n",
    "            if test_prediction == actual_label:\n",
    "                test_accuracy_count += 1\n",
    "\n",
    "        test_accuracy = (100 / len(test_x)) * test_accuracy_count\n",
    "        testing_accuracies.append(test_accuracy)\n",
    "        \n",
    "           \n",
    "        \n",
    "        print(\"Epoch: \" + str(epoch) +\n",
    "              \" Error: \" + str(round(error, 5)) +\n",
    "              \" Accuracy: \" + str(accuracy) + \"%\" +\n",
    "              \" Test Error: \" + str(round(test_error, 5)) +\n",
    "              \" Accuracy: \" + str(test_accuracy) + \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}