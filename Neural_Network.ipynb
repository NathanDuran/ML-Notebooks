{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "As we have seen, Perceptrons are only capable of solving *linearly separable* problems.\n",
    "To overcome this limitation we can connect Perceptrons together into a network, first proposed by Rumelhart. Mclelland & Hinton (1980's).\n",
    "Each one becomes a *Node* in the network and they are connected together into *Layers*.\n",
    "In standard Artificial Neural Network (ANN) architecture there is one *input*, one *output* and one or more *hidden* layers.\n",
    "Though *input* layer is a bit misleading, it doesn't actually do any computation, it is just the inputs to the network.\n",
    "\n",
    "![ANN](resources/ANN.png \"ANN Image\")\n",
    "\n",
    "So outputs of hidden layers become the inputs to subsequent hidden layers, or the output layer if it is the last hidden layer.\n",
    "Hidden nodes tend to learn different aspects of the problem space,\n",
    "building more complex decision boundaries and are therefore able to solve more complex problems.\n",
    "To give you an intuition from the XOR problem we just saw, take a look at the following diagram.\n",
    "The hidden nodes both learn different logical functions (AND and OR), if you combine these you have solved XOR!\n",
    "\n",
    "![ANN-XOR](resources/ANN-XOR.png \"ANN-XOR Image\")\n",
    "\n",
    "**Note:** The number of nodes in the input layer *must* equal the number of inputs/features in the data.\n",
    "The number of output nodes *must* equal the number of labels/classes in the data.\n",
    "The number of hidden layers and nodes in the layers is arbitrary,\n",
    "and selecting this architecture is part of building an ANN.\n",
    "\n",
    "### Differences Between Perceptrons and ANN\n",
    "\n",
    "Before we look at the algorithm for ANN we need to understand two key differences.\n",
    "\n",
    "#### 1. Activation Function\n",
    "\n",
    "We need each node to output a *real number*,\n",
    "so the step function we used before (which outputs 0 or 1) will not work.\n",
    "Instead we use the Sigmoid function, which 'squashes' the output into a real number between 0 and 1.\n",
    "\n",
    "**Note:** Other activation functions are also used, such as Tahn and ReLu, but we will stick to Sigmoid.\n",
    "\n",
    "![Activation-Functions](resources/Activation-Functions.png \"Activation-Functions Image\")\n",
    "\n",
    "We need real numbers because:\n",
    "1. For output nodes, real numbers between 0 and 1 can be considered a *probability* of an input example belonging to a particular class.\n",
    "2. Hidden layer nodes need to produce *some* output, even if it is very small,\n",
    "so that we can calculate the error and update weights using Backpropagation.\n",
    "\n",
    "#### 2. Backpropagation and Gradient Descent\n",
    "\n",
    "With a Perceptron we only have one layer, so,\n",
    "from its output we can calculate the error it produces and use that to update the weight values.\n",
    "But now we have multiple layers what should the hidden nodes output be?\n",
    "What is the error and how much should we change the weights?\n",
    "\n",
    "Instead, we *share out the error* from the output nodes to the hidden nodes,\n",
    "and we do this in *proportion to the strength of the 'signal'* (output) that it produced - hence why we need *some* output.\n",
    "So we are *propagating* the error from the output nodes back up the network.\n",
    "This is achieved by calculating the derivative of the error from the previous layer with respect to the weights (derivative of the error function).\n",
    "We then use a similar weight update function that we did with Perceptrons:\n",
    "\n",
    "error(derivative of error function) X input X learning rate\n",
    "\n",
    "Why do we calculate the derivative of the error function? This is an algorithm called **Stochastic Gradient Descent**.\n",
    "We want to *minimise* the error produced by a weight.\n",
    "By calculating the derivative we get the *gradient* or the 'steepness' of a curve at that point (weight value).\n",
    "The larger the gradient the further we are from the minimum error (0 gradient).\n",
    "Again, the learning rate is how large a step we want to take towards the minimum error.\n",
    "\n",
    "![Gradient-Descent](resources/Gradient-Descent.png \"Gradient-Descent Image\")\n",
    "\n",
    "THE LEARNING RATE IS NEGATIVE?\n",
    "https://www.kaggle.com/niyipop/2-layer-neural-network-from-scratch-using-numpy\n",
    "### ANN Algorithm\n",
    "\n",
    "Similar to Perceptrons, ANN are trained in two 'phases'.\n",
    "The forward pass, where data is input into the network to produce an output.\n",
    "The backward pass, where the error in output is used to update the weights using Backpropagation and Gradient Descent.\n",
    "```\n",
    "1. Set weights to random values in range [-0.5, 0.5]\n",
    "\n",
    "2. Set learning rate to a small value, usually less than 0.5\n",
    "\n",
    "3. For each training example in the dataset i.e one 'epoch'\n",
    "\n",
    "    Forward Propagation\n",
    "    \n",
    "    For each node in the layer and each layer in turn:\n",
    "        \n",
    "        A. Sum inputs multiplied by weights\n",
    "    \n",
    "        B. Calculate Sigmoid (activation) of the sum\n",
    "    \n",
    "    Backpropagation\n",
    "    \n",
    "        C. For each node in the layer and each layer in turn **going backwards**:\n",
    "        \n",
    "        Calculate the error and derivative, first the output layer then hidden.\n",
    "        \n",
    "            output error = expected/desired output - activation\n",
    "    \n",
    "            output derivative = output error X derivative of sigmoid for layers output\n",
    "    \n",
    "            hidden layer error = output derivative X output weights\n",
    "        \n",
    "            hidden layer derivative = hidden layer error X derivative of sigmoid for layers output\n",
    "        \n",
    "        D. Update all the weights **at the same time**:\n",
    "    \n",
    "        change in weight = layer derivative X input X learning rate\n",
    "    \n",
    "4. Repeat from step 3 until error is as small as possible, or (more likely) for the number of training epochs.\n",
    "```\n",
    "\n",
    "Or if you prefer maths...\n",
    "\n",
    "Forward Propagation:\n",
    "\n",
    "$a = \\sigma ( \\sum\\limits_{i=0}^{n} w_i \\times x_i)$\n",
    "\n",
    "Backpropagation:\n",
    "\n",
    "$ w^{\\prime} = w - \\alpha\\frac{\\partial E}{\\partial w}$\n",
    "\n",
    "### Iris Dataset\n",
    "\n",
    "The Iris Flower data set contains 150 examples from three species Iris Setosa, Iris Virginica and Iris Versicolor.\n",
    "There are 50 examples of each species and each example has four measurements (features) Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
    "The Iris data is often used as an example for machine learning classifiers and we are going to build and test an ANN to classify the data\n",
    "i.e. given a set of measurements, what is the species?\n",
    "\n",
    "![Iris-image](resources/Iris-image.png \"Iris-image Image\")\n",
    "\n",
    "Real data very rarely comes in a format that is suitable for input to a machine learning algorithm.\n",
    "So first we need to prepare the data ready for classification.\n",
    "It is also often useful to visualise the data because this might help us select what kind of classifier is suitable and predict how well they might perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Read data from csv\n",
    "iris = pd.read_csv(\"data/Iris.csv\")\n",
    "print(iris.head())\n",
    "\n",
    "# Plot the various combinations of 2D graph\n",
    "g = sns.pairplot(iris.drop(\"Id\", axis=1), hue=\"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Next we need to replace the species labels with numbers and convert them to numbers.\n",
    "In this case we are going to use ‘one-hot encoding’,\n",
    "which means each species label will be replaced with a set of binary values which indicate which of the three species it is\n",
    "i.e 'Iris-setosa' = 1 0 0, 'Iris-versicolor' = 0 1 0 and 'Iris-virginica' = 0 0 1.\n",
    "\n",
    "We also need to get all of the features from the relevant columns and split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Replace the species with 0, 1 or 2 as appropriate\n",
    "iris['Species'].replace(['Iris-setosa', 'Iris-virginica', 'Iris-versicolor'], [0, 1, 2], inplace=True)\n",
    "\n",
    "# Get labels, flatten and encode to one-hot\n",
    "columns = ['Species']\n",
    "labels = pd.DataFrame(iris, columns=columns).to_numpy()\n",
    "labels = labels.flatten()\n",
    "labels = np.eye(np.max(labels) + 1)[labels]\n",
    "\n",
    "# Get Features\n",
    "columns = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
    "features = pd.DataFrame(iris, columns=columns).to_numpy()\n",
    "\n",
    "# Split data to training and test data, 2/3 for training and 1/3 for testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Build the ANN\n",
    "First we will define the Sigmoid activation function and its derivative.\n",
    "Then, as before, we will define a few parameters such as learning rate and number of training epochs.\n",
    "\n",
    "The main difference between our Perceptron implementation is that we will use matrices to represent our layer weights.\n",
    "If we had to manually add new variables for each weight/node it would be quite unmanageable.\n",
    "For example with 4 inputs and 6 hidden nodes (+ 6 bias) = 30 weight variables for just one layer.\n",
    "Instead we can represent the entire layer as a matrix, in this case the hidden layer will be a 4x6 matrix.\n",
    "This also allows us to perform the calculations on the entire layer at once, rather than using loops.\n",
    "\n",
    "The rest of the code is the implementation of the forward and backward passes through the network.\n",
    "Every 100 epochs we will record the mean squared error and accuracy of predictions.\n",
    "You should see the error drop and accuracy increase smoothly(ish) over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sigmoid and its derivative\n",
    "def sigmoid(x):\n",
    "    # This is not strictly sigmoid, but more stable when handling matrices\n",
    "    return .5 * (1 + np.tanh(.5 * x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 2000\n",
    "\n",
    "# Network architecture parameters\n",
    "num_features = len(train_x[0])\n",
    "num_classes = len(train_y[0])\n",
    "num_hidden_nodes = 6\n",
    "\n",
    "# Initialise weights in the range -1 to 1\n",
    "np.random.seed(1)\n",
    "# Hidden layer weights with shape = number of input features x number of hidden nodes\n",
    "hidden_weights = np.random.uniform(-1, 1, size=(num_features, num_hidden_nodes))\n",
    "hidden_bias = np.random.uniform(-1, 1, size=(1, num_hidden_nodes))\n",
    "# Output layer weights with shape = number of hidden nodes x number of output classes\n",
    "output_weights = np.random.uniform(-1, 1, size=(num_hidden_nodes, num_classes))\n",
    "output_bias = np.random.uniform(-1, 1, size=(1, num_classes))\n",
    "\n",
    "# For recording error and accuracy - for graph later\n",
    "training_errors = []\n",
    "testing_errors = []\n",
    "training_accuracies = []\n",
    "testing_accuracies = []\n",
    "\n",
    "# Train for number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Forward pass\n",
    "    input_layer = train_x\n",
    "    # sigmoid( (W * X) + b)\n",
    "    hidden_layer = sigmoid(np.dot(input_layer, hidden_weights) + hidden_bias)\n",
    "    output_layer = sigmoid(np.dot(hidden_layer, output_weights) + output_bias)\n",
    "\n",
    "    # Backpropagation using gradient descent\n",
    "    # Calculate output layer error\n",
    "    output_layer_error = train_y - output_layer\n",
    "    # Calculate output layer derivative Note: that we just need this layers error for the bias\n",
    "    output_layer_delta = output_layer_error * sigmoid_deriv(output_layer)\n",
    "    output_bias_delta = np.sum(output_layer_error, axis=0)\n",
    "    \n",
    "    # Calculate hidden layer error (from the output layers weights and derivative\n",
    "    hidden_layer_error = output_layer_delta.dot(output_weights.T)\n",
    "    # Calculate hidden layer derivative Note: that we just need this layers error for the bias\n",
    "    hidden_layer_delta = hidden_layer_error * sigmoid_deriv(hidden_layer)\n",
    "    hidden_bias_delta = np.sum(hidden_layer_error, axis=0)\n",
    "\n",
    "    # Update the weights (learning rate X layers input X layers derivative)\n",
    "    output_weights += learning_rate * hidden_layer.T.dot(output_layer_delta)\n",
    "    output_bias += learning_rate * output_bias_delta\n",
    "    \n",
    "    hidden_weights += learning_rate * input_layer.T.dot(hidden_layer_delta)\n",
    "    hidden_bias += learning_rate * hidden_bias_delta\n",
    "\n",
    "    # Every 100 epochs record error and accuracy during training\n",
    "    if (epoch % 100) == 0:\n",
    "        \n",
    "        # Mean squared error over all errors this epoch\n",
    "        error = 0.5 * np.mean(np.abs(output_layer_error)) ** 2\n",
    "        training_errors.append(error)\n",
    "\n",
    "        accuracy_count = 0\n",
    "        for i in range(len(output_layer)):\n",
    "          \n",
    "            # Get the prediction i.e. the output with the highest value\n",
    "            prediction = np.argmax(output_layer[i])\n",
    "            # Get the actual label\n",
    "            actual_label = np.argmax(train_y[i])\n",
    "            \n",
    "            # If they match the prediction was correct\n",
    "            if prediction == actual_label:\n",
    "                accuracy_count += 1\n",
    "        accuracy = (len(train_x) / 100) * accuracy_count\n",
    "        training_accuracies.append(accuracy)\n",
    "        \n",
    "        ##YOUR CODE STARTS HERE##     \n",
    "        \n",
    "        # Forward pass\n",
    "        test_hidden = sigmoid(np.dot(test_x, hidden_weights) + hidden_bias)\n",
    "        test_output = sigmoid(np.dot(test_hidden, output_weights) + output_bias)\n",
    "        test_output_error = test_y - test_output\n",
    "        # Mean squared error over all errors\n",
    "        test_error = 0.5 * np.mean(np.abs(test_output_error)) ** 2\n",
    "        testing_errors.append(test_error)\n",
    "\n",
    "        test_accuracy_count = 0\n",
    "        for j in range(len(test_output)):\n",
    "\n",
    "            # Get the prediction i.e. the output with the highest value\n",
    "            test_prediction = np.argmax(test_output[j])\n",
    "            # Get the actual label\n",
    "            actual_label = np.argmax(test_y[j])\n",
    "\n",
    "            # If they match the prediction was correct\n",
    "            if test_prediction == actual_label:\n",
    "                test_accuracy_count += 1\n",
    "        test_accuracy = (100 / len(test_x)) * test_accuracy_count\n",
    "        testing_accuracies.append(test_accuracy)\n",
    "        \n",
    "           \n",
    "        \n",
    "        print(\"Epoch: \" + str(epoch) +\n",
    "              \" Error: \" + str(round(error, 5)) +\n",
    "              \" Accuracy: \" + str(accuracy) + \"%\" +\n",
    "              \" Test Error: \" + str(round(test_error, 5)) +\n",
    "              \" Accuracy: \" + str(test_accuracy) + \"%\")\n",
    "        ##YOUR CODE ENDS HERE##\n",
    "        # print(\"Epoch: \" + str(epoch) +\n",
    "        #       \" Error: \" + str(round(error, 5)) +\n",
    "        #       \" Accuracy: \" + str(accuracy) + \"%\")\n",
    "        \n",
    "# Plot the error chart\n",
    "plt.plot(training_errors)\n",
    "plt.plot(testing_errors)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy chart\n",
    "plt.plot(training_accuracies)\n",
    "plt.plot(testing_accuracies)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Testing the Network\n",
    "You should see quite a high accuracy and low error on the training set.\n",
    "Though in reality this is not a good measure of how well the network will perform\n",
    "because it has already 'seen' the training data and used that to adjust the weights.\n",
    "How well would it perform on data that it has not seen? How well will it 'generalise' to new data?\n",
    "\n",
    "This is why we kept 1/3rd of our data for testing!\n",
    "\n",
    "Lets apply our learned model to the test set and see how well it is able to classify flowers that it has not seen before.\n",
    "You can either do this in a new cell below,\n",
    "or (recommended) add code above in the section where we are measuring error and accuracy every 100 epochs.\n",
    "That way we can track progress as the network trains.\n",
    "You can even add the testing errors and accuracies to the plots for comparison.\n",
    "\n",
    "So you need to:\n",
    "1. Perform a forward pass **only**, no updating weights, using the test inputs.\n",
    "2. Calculate the error on the test labels after the forward pass.\n",
    "3. Calculate the accuracy on the test set.\n",
    "\n",
    "**Note:** It is recommended that you create new variables for everything **except** the weight/bias matrices.\n",
    "Those matrices are the model we have trained.\n",
    "\n",
    "You can also try different learning rates/epochs or anything else to see what effect it has.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}